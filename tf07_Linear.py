# y = wx + b
from pickletools import optimize
import tensorflow as tf
tf.set_random_seed(66)


#.1 DATA
x_train = [1,2,3]
y_train = [1,2,3]

w = tf.Variable(1, dtype=tf.float32)
b = tf.Variable(1, dtype=tf.float32)


#.2 MODEL
hypothesis = x_train * w + b   # y = wx + b


#.3-1 COMPILE 
loss = tf.reduce_mean(tf.square(hypothesis - y_train)) # MSE
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train = optimizer.minimize(loss) # optimizer='sgd'
# model.copile(loss='mse', optimizer='sgd')


#.3-2 TRAIN
sess = tf.compat.v1.Session()
sess.run(tf.global_variables_initializer())

for step in range(2001):
    sess.run(train)
    if step % 20 ==0:
        print(step, sess.run(loss), sess.run(w), sess.run(b))

        
'''
0 0.8110667 0.96 0.98
20 0.09747055 0.68079156 0.8102924
40 0.08272933 0.66902816 0.7604377
60 0.07508357 0.682034 0.7235789
80 0.068191625 0.6967346 0.6894666
100 0.061932724 0.7109639 0.65705407
120 0.05624826 0.7245453 0.626174
140 0.051085588 0.7374904 0.596746
160 0.046396747 0.7498274 0.56870115
180 0.04213832 0.7615845 0.5419744
200 0.038270667 0.7727892 0.5165037
220 0.03475807 0.7834672 0.49222997
240 0.03156784 0.7936434 0.46909702
260 0.02867041 0.8033415 0.44705117
280 0.02603892 0.8125837 0.42604136
300 0.02364896 0.8213916 0.406019
320 0.02147836 0.82978547 0.3869376
340 0.019506995 0.837785 0.36875296
360 0.017716562 0.84540844 0.35142294
380 0.016090473 0.8526737 0.3349073
400 0.014613622 0.85959756 0.31916788
420 0.013272323 0.86619586 0.30416816
440 0.012054126 0.87248427 0.28987336
460 0.01094775 0.87847704 0.2762503
480 0.00994292 0.8841882 0.2632675
500 0.009030321 0.889631 0.25089484
520 0.00820147 0.8948179 0.2391037
540 0.007448713 0.899761 0.22786674
560 0.0067650415 0.9044719 0.21715784
580 0.0061441152 0.90896136 0.20695223
600 0.0055801845 0.9132399 0.19722626
620 0.0050680176 0.9173173 0.18795733
640 0.0046028537 0.9212031 0.17912397
660 0.00418039 0.9249062 0.17070584
680 0.0037967034 0.9284352 0.16268332
700 0.0034482165 0.9317986 0.15503784
720 0.003131728 0.9350038 0.14775163
740 0.0028442843 0.9380583 0.14080784
760 0.002583229 0.9409694 0.13419041
780 0.002346135 0.9437436 0.12788399
800 0.002130795 0.94638747 0.12187389
820 0.0019352174 0.9489071 0.11614626
840 0.0017575937 0.95130825 0.11068777
860 0.0015962725 0.9535966 0.10548585
880 0.00144976 0.9557774 0.10052839
900 0.0013166973 0.95785564 0.09580395
920 0.0011958504 0.95983636 0.09130153
940 0.0010860872 0.96172386 0.08701065
960 0.0009863975 0.96352273 0.08292144
980 0.0008958689 0.965237 0.079024434
1000 0.0008136393 0.9668708 0.07531057
1020 0.00073896087 0.96842766 0.07177125
1040 0.000671136 0.9699114 0.06839835
1060 0.0006095381 0.97132546 0.065183885
1080 0.000553593 0.9726731 0.062120494
1100 0.0005027807 0.97395736 0.059201043
1120 0.00045663046 0.9751812 0.05641883
1140 0.00041472193 0.9763476 0.053767353
1160 0.00037665654 0.9774592 0.051240496
1180 0.0003420842 0.97851855 0.048832376
1200 0.00031068941 0.9795281 0.04653744
1220 0.00028216987 0.98049027 0.04435027
1240 0.00025627288 0.98140717 0.042265963
1260 0.00023275013 0.98228097 0.0402796
1280 0.00021138787 0.9831137 0.038386583
1300 0.00019198499 0.9839073 0.03658255
1320 0.00017436403 0.98466355 0.034863316
1340 0.00015836056 0.98538435 0.033224843
1360 0.00014382433 0.9860712 0.031663377
1380 0.00013062467 0.9867258 0.030175328
1400 0.000118634845 0.9873496 0.028757207
1420 0.00010774505 0.9879442 0.02740573
1440 9.785653e-05 0.9885107 0.026117763
1460 8.8874716e-05 0.9890507 0.02489036
1480 8.0718e-05 0.9895652 0.023720639
1500 7.330924e-05 0.99005574 0.022605825
1520 6.658002e-05 0.9905231 0.021543395
1540 6.0469454e-05 0.99096847 0.0205309
1560 5.4918462e-05 0.9913929 0.019565996
1580 4.9878097e-05 0.9917974 0.01864646
1600 4.530067e-05 0.9921829 0.017770145
1620 4.114256e-05 0.99255025 0.016935011
1640 3.736659e-05 0.9929004 0.016139137
1660 3.3936896e-05 0.99323404 0.015380644
1680 3.0821753e-05 0.993552 0.014657802
1700 2.7992872e-05 0.993855 0.013968946
1720 2.5423387e-05 0.9941438 0.013312481
1740 2.3090353e-05 0.99441904 0.012686847
1760 2.0970465e-05 0.99468136 0.012090621
1780 1.9045985e-05 0.9949312 0.011522437
1800 1.7298553e-05 0.9951695 0.0109809255
1820 1.5710277e-05 0.9953965 0.010464847
1840 1.4268378e-05 0.99561286 0.009973031
1860 1.2959122e-05 0.99581903 0.009504335
1880 1.1769396e-05 0.9960155 0.009057664
1900 1.0689179e-05 0.99620277 0.008632005
1920 9.708013e-06 0.9963812 0.008226337
1940 8.816933e-06 0.9965513 0.007839731
1960 8.007689e-06 0.99671334 0.007471312
1980 7.2730195e-06 0.99686784 0.007120189
2000 6.6050598e-06 0.99701506 0.006785556
'''